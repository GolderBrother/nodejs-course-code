# 本地模型和本地知识库构建AI小说助手

## 跑本地模型

```bash
ollama run qwen2.5:1.5b
```

## 总结

通过本地知识库、本地 AI 模型，并用它来做了一个阅读小说的助手。

AI 模型只知道公开的信息，如果想让它对一些你的私有信息做解读，就需要用到知识库了。

我们用 docker 跑了 maxkb 这个开源知识库，完全免费用，数据都是存在本地的相当安全。

你可以用它接入线上 AI 模型来用，但如果担心这样不安全，也可以用 Ollma 跑个本地 AI 模型。

我们试了下本地 Ollama 跑的 qwen2.5 模型，然后搭配小说知识库里的一篇小说，续写啥的完全不成问题，解读也很到位。

当然，这些不是前端领域的应用。

对前端来说，最有价值的是把内部的组件库、文档等做成知识库，让 AI 生成的代码用内部组件库来写，也可以让它来做文档查询助手。

本地的 AI 模型 + 本地的知识库，完全不用花一分钱，但效果也足够用。